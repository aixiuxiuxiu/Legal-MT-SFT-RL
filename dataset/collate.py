import torch

# Unsloth needs to be loaded first so that unsloth_zoo works correctly.
# Great side effects... and without a real dependence in the first place.
import unsloth
from transformers import PreTrainedTokenizerBase
from unsloth_zoo import vision_utils

from .batch import Batch
from .chat.processing import MessageBoundaries
from .instruct import InstructSample

assert unsloth.unsloth_zoo is not None, (
    "Unsloth zoo was not loaded correctly. Probably not your fault, but just some "
    "nonsense check from unsloth, as there is a lot of terrible library design."
)


class InstructCollator:
    def __init__(
        self,
        processor: PreTrainedTokenizerBase,
        ignore_index: int = -100,
        assistant_only: bool = True,
        include_answer: bool = True,
        prefill: str | None = None,
    ):
        """
        Args:
            processor (PreTrainedTokenizerBase): Processor of the model.
            ignore_index (int): Label value that is ignored in the loss. [Default: -100]
            assistant_only (bool): Whether only the assistant messages should be
                included in the labels. [Default: True]
            include_answer (bool): Whether the answer should be included. If it is not
                included, a generation prompt will be added to the end, which is used
                for inference / validation.
                [Default: True]
        """
        if include_answer and prefill is not None:
            raise ValueError("Cannot use `prefill` together with `include_answer=True`")
        self.processor = processor
        self.padding_token_ids = vision_utils.get_padding_tokens_ids(self.processor)
        self.ignore_index = ignore_index
        self.assistant_boundaries = (
            MessageBoundaries.identify_assistant(self.processor)
            if assistant_only
            else None
        )
        self.include_answer = include_answer
        self.prefill = prefill

    def __call__(self, samples: list[InstructSample]) -> Batch:
        messages = [
            self.processor.apply_chat_template(
                # HuggingFace got the type annotations wrong of the chat messages.
                sample.as_chat(
                    include_answer=self.include_answer,  # pyright: ignore[reportArgumentType]
                    prefill=self.prefill,
                ),
                tokenize=False,
                add_generation_prompt=not self.include_answer and self.prefill is None,
                continue_final_message=self.prefill is not None,
            )
            for sample in samples
        ]
        images = [sample.get_images() for sample in samples]
        has_images = any(len(imgs) > 0 for imgs in images)
        # The images are put into a dict, because some processors don't support the
        # images argument, which are usually text only models. Hence setting it to
        # None would not work for those.
        extra_args = dict(images=images) if has_images else {}

        batch = self.processor(
            # That type might be wrong because processor may be different from the
            # tokeniser (as there is no actual base class for the processor to use).
            text=messages,  # pyright: ignore[reportArgumentType]
            padding=True,
            return_tensors="pt",
            **extra_args,
        )

        labels: torch.Tensor = batch.input_ids.clone()
        labels[torch.isin(labels, self.padding_token_ids)] = self.ignore_index
        if self.assistant_boundaries:
            # The assistant mask is True for all tokens that would be generated by the
            # model during inference, this means that it is everything after the opening
            # chat sequence (e.g. <|im_start|>assistant\n) up to and including the end
            # of sequence (e.g. <|im_end|>).
            assistant_mask = self.assistant_boundaries.mask(labels)
            # All tokens that are not part of the assistant message will be ignored.
            labels[~assistant_mask] = -100
        # This looks quite odd that the labels are exactly the inputs, but I believe
        # that HuggingFace's models automatically shift the labels by one for the loss.
        # At least all the examples I have seen would suggest that.
        batch["labels"] = labels

        info = {}
        for sample in samples:
            for key, value in sample.info.items():
                if key not in info:
                    info[key] = []
                info[key].append(value)

        return Batch(
            data=batch,
            answers=[sample.answer for sample in samples],
            info=info,
        )
