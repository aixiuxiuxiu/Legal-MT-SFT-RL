import torch
from transformers import BatchEncoding, PreTrainedTokenizerBase
from unsloth_zoo import vision_utils

from .chat.processing import MessageBoundaries
from .instruct import InstructSample


class InstructCollator:
    def __init__(
        self,
        processor: PreTrainedTokenizerBase,
        ignore_index: int = -100,
        assistant_only: bool = True,
    ):
        """
        Args:
            processor (PreTrainedTokenizerBase): Processor of the model.
            ignore_index (int): Label value that is ignored in the loss. [Default: -100]
            assistant_only (bool): Whether only the assistant messages should be
                included in the labels. [Default: True]
        """
        self.processor = processor
        self.padding_token_ids = vision_utils.get_padding_tokens_ids(self.processor)
        self.ignore_index = ignore_index
        self.assistant_boundaries = (
            MessageBoundaries.identify_assistant(self.processor)
            if assistant_only
            else None
        )

    def __call__(self, samples: list[InstructSample]) -> BatchEncoding:
        messages = [
            self.processor.apply_chat_template(
                # HuggingFace got the type annotations wrong of the chat messages.
                sample.as_chat(),  # pyright: ignore[reportArgumentType]
                tokenize=False,
            )
            for sample in samples
        ]
        images = [sample.get_images() for sample in samples]

        batch = self.processor(
            # That type might be wrong because processor may be different from the
            # tokeniser (as there is no actual base class for the processor to use).
            text=messages,  # pyright: ignore[reportArgumentType]
            images=images,
            padding=True,
            return_tensors="pt",
        )

        labels: torch.Tensor = batch.input_ids.clone()
        labels[torch.isin(labels, self.padding_token_ids)] = self.ignore_index
        if self.assistant_boundaries:
            # The assistant mask is True for all tokens that would be generated by the
            # model during inference, this means that it is everything after the opening
            # chat sequence (e.g. <|im_start|>assistant\n) up to and including the end
            # of sequence (e.g. <|im_end|>).
            assistant_mask = self.assistant_boundaries.mask(labels)
            # All tokens that are not part of the assistant message will be ignored.
            labels[~assistant_mask] = -100
        # This looks quite odd that the labels are exactly the inputs, but I believe
        # that HuggingFace's models automatically shift the labels by one for the loss.
        # At least all the examples I have seen would suggest that.
        batch["labels"] = labels

        return batch
